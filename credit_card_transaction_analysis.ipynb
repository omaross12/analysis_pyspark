{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework 2 Submission\n",
    "\n",
    "### Student Number: 210590034"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as sf\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "from pyspark.sql import types\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://lena-master:4044\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>fraud</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f65501c2b80>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create spark session\n",
    "sc = SparkSession.builder.appName(\"fraud\").getOrCreate()\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import .csv from hdfs\n",
    "df_train = sc.read.csv('fraudTrain.csv', header=True, inferSchema = True)\n",
    "df_test = sc.read.csv('fraudTest.csv', header=True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in train: 1296675\n",
      "Number of rows in test: 555719\n"
     ]
    }
   ],
   "source": [
    "#count number of rows of our dataframes\n",
    "num_rows_train = df_train.count()\n",
    "num_rows_test = df_test.count()\n",
    "print(f'Number of rows in train: {num_rows_train}')\n",
    "print(f'Number of rows in test: {num_rows_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- trans_date_trans_time: string (nullable = true)\n",
      " |-- cc_num: long (nullable = true)\n",
      " |-- merchant: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- amt: double (nullable = true)\n",
      " |-- first: string (nullable = true)\n",
      " |-- last: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- street: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- zip: integer (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- long: double (nullable = true)\n",
      " |-- city_pop: integer (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- trans_num: string (nullable = true)\n",
      " |-- unix_time: integer (nullable = true)\n",
      " |-- merch_lat: double (nullable = true)\n",
      " |-- merch_long: double (nullable = true)\n",
      " |-- is_fraud: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# combining both train and test sets for data analysis\n",
    "df = df_train.union(df_test)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 1852394\n"
     ]
    }
   ],
   "source": [
    "#count number of rows of our combined dataFrame\n",
    "num_rows = df.count()\n",
    "print(f'Number of rows: {num_rows}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of original data rows:  1852394\n",
      "number of data rows after deleting duplicated data:  1852394\n",
      "number of duplicate rows:  0\n"
     ]
    }
   ],
   "source": [
    "#count the number of original data rows\n",
    "df_row_count =df.count()\n",
    "print(\"number of original data rows: \", df_row_count)\n",
    "#count the number of data rows after deleting duplicated data\n",
    "df_duplicate_count = df.dropDuplicates().count()\n",
    "print(\"number of data rows after deleting duplicated data: \", df_duplicate_count)\n",
    "differenece = df_row_count - df_duplicate_count\n",
    "print(\"number of duplicate rows: \", differenece)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rows with missing data:  0\n"
     ]
    }
   ],
   "source": [
    "# use how=\"all\" for missing data in the entire column\n",
    "df_missing = df.dropDuplicates().dropna(how=\"any\")\n",
    "df_missing_any = df_row_count - df_missing.count()\n",
    "print(\"number of rows with missing data: \", df_missing_any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------------------\n",
      " _c0                   | 0                    \n",
      " trans_date_trans_time | 2019-01-01 00:00:18  \n",
      " cc_num                | 2703186189652095     \n",
      " merchant              | fraud_Rippin, Kub... \n",
      " category              | misc_net             \n",
      " amt                   | 4.97                 \n",
      " first                 | Jennifer             \n",
      " last                  | Banks                \n",
      " gender                | F                    \n",
      " street                | 561 Perry Cove       \n",
      " city                  | Moravian Falls       \n",
      " state                 | NC                   \n",
      " zip                   | 28654                \n",
      " lat                   | 36.0788              \n",
      " long                  | -81.1781             \n",
      " city_pop              | 3495                 \n",
      " job                   | Psychologist, cou... \n",
      " dob                   | 1988-03-09           \n",
      " trans_num             | 0b242abb623afc578... \n",
      " unix_time             | 1325376018           \n",
      " merch_lat             | 36.011293            \n",
      " merch_long            | -82.048315           \n",
      " is_fraud              | 0                    \n",
      " full_name             | Jennifer Banks       \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# combine 'first' and 'last' columns to create 'full_name column'\n",
    "df = df.withColumn('full_name', sf.concat(sf.col('first'),sf.lit(' '), sf.col('last')))\n",
    "df.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|is_fraud|  count|\n",
      "+--------+-------+\n",
      "|       1|   9651|\n",
      "|       0|1842743|\n",
      "+--------+-------+\n",
      "\n",
      "Fraud percentage = 0.005210014716091717\n",
      "Legitamite percentage = 0.9947899852839083\n"
     ]
    }
   ],
   "source": [
    "# checking balance of fraud cases (target variable)\n",
    "df.select(\"is_fraud\").groupBy(\"is_fraud\").count().show()\n",
    "fraud_count = df.select(\"is_fraud\").where(df.is_fraud == '1').count()\n",
    "legit_count = df.select(\"is_fraud\").where(df.is_fraud == '0').count()\n",
    "print(f'Fraud percentage = {fraud_count / df_row_count}')\n",
    "print(f'Legitamite percentage = {legit_count / df_row_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is highly imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct customers: 989\n",
      "Number of distinct cards: 999\n",
      "Number of distinct merchants: 693\n",
      "Number of distinct categories: 14\n",
      "Number of distinct streets: 999\n",
      "Number of distinct cities: 906\n",
      "Number of distinct states: 51\n",
      "Number of distinct zip codes: 985\n",
      "Number of distinct latitudes: 983\n",
      "Number of distinct longitudes: 983\n",
      "Number of distinct jobs: 497\n",
      "Number of distinct transaction numbers: 1852394\n",
      "Number of distinct dates of birth: 984\n"
     ]
    }
   ],
   "source": [
    "# number of unique categorical values\n",
    "customers = df.select('full_name').distinct().count()\n",
    "cards = df.select('cc_num').distinct().count()\n",
    "merchant = df.select('merchant').distinct().count()\n",
    "category = df.select('category').distinct().count()\n",
    "street = df.select('street').distinct().count()\n",
    "city = df.select('city').distinct().count()\n",
    "state = df.select('state').distinct().count()\n",
    "zip_codes = df.select('zip').distinct().count()\n",
    "lat = df.select('lat').distinct().count()\n",
    "long = df.select('long').distinct().count()\n",
    "job = df.select('job').distinct().count()\n",
    "trans_num = df.select('trans_num').distinct().count()\n",
    "dob = df.select('dob').distinct().count()\n",
    "\n",
    "print(f'Number of distinct customers: {customers}')\n",
    "print(f'Number of distinct cards: {cards}')\n",
    "print(f'Number of distinct merchants: {merchant}')\n",
    "print(f'Number of distinct categories: {category}')\n",
    "print(f'Number of distinct streets: {street}')\n",
    "print(f'Number of distinct cities: {city}')\n",
    "print(f'Number of distinct states: {state}')\n",
    "print(f'Number of distinct zip codes: {zip_codes}')\n",
    "print(f'Number of distinct latitudes: {lat}')\n",
    "print(f'Number of distinct longitudes: {long}')\n",
    "print(f'Number of distinct jobs: {job}')\n",
    "print(f'Number of distinct transaction numbers: {trans_num}')\n",
    "print(f'Number of distinct dates of birth: {dob}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|gender|  count|\n",
      "+------+-------+\n",
      "|     F|1014749|\n",
      "|     M| 837645|\n",
      "+------+-------+\n",
      "\n",
      "Female percentage = 0.5478040848761117\n",
      "Male percentage = 0.45219591512388835\n"
     ]
    }
   ],
   "source": [
    "# checking balance of male and female customers\n",
    "df.select(\"gender\").groupBy(\"gender\").count().show()\n",
    "female_count = df.select(\"gender\").where(df.gender == 'F').count()\n",
    "male_count = df.select(\"gender\").where(df.gender == 'M').count()\n",
    "print(f'Female percentage = {female_count / df_row_count}')\n",
    "print(f'Male percentage = {male_count / df_row_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use 'trans_date_trans_time' column to create column with the only the required info from the time (month, day, hour)\n",
    "df = df.withColumn(\"trans_date_trans_time\", sf.to_timestamp(\"trans_date_trans_time\", 'yyyy-MM-dd HH:mm:ss'))\n",
    "df = df.withColumn('month', sf.month(df.trans_date_trans_time))\n",
    "df = df.withColumn('day', sf.date_format(df.trans_date_trans_time, \"d\"))\n",
    "df = df.withColumn('hour', sf.hour(df.trans_date_trans_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# caluclate customer age at the time of transaction\n",
    "df = df.withColumn(\"dob\", sf.to_timestamp(\"dob\", 'yyyy-MM-dd'))\n",
    "df = df.withColumn(\"age_at_transaction\",sf.round(sf.months_between(sf.col(\"trans_date_trans_time\"),sf.col(\"dob\"))/sf.lit(12)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unnecessary columns\n",
    "df = df.drop('_c0','first','last','street','zip','lat','long','dob','trans_num','merch_lat',\n",
    "             'merch_long','unix_time','trans_date_trans_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cc_num</th>\n",
       "      <th>merchant</th>\n",
       "      <th>category</th>\n",
       "      <th>amt</th>\n",
       "      <th>gender</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>city_pop</th>\n",
       "      <th>job</th>\n",
       "      <th>is_fraud</th>\n",
       "      <th>full_name</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>age_at_transaction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2703186189652095</td>\n",
       "      <td>fraud_Rippin, Kub and Mann</td>\n",
       "      <td>misc_net</td>\n",
       "      <td>4.97</td>\n",
       "      <td>F</td>\n",
       "      <td>Moravian Falls</td>\n",
       "      <td>NC</td>\n",
       "      <td>3495</td>\n",
       "      <td>Psychologist, counselling</td>\n",
       "      <td>0</td>\n",
       "      <td>Jennifer Banks</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>630423337322</td>\n",
       "      <td>fraud_Heller, Gutmann and Zieme</td>\n",
       "      <td>grocery_pos</td>\n",
       "      <td>107.23</td>\n",
       "      <td>F</td>\n",
       "      <td>Orient</td>\n",
       "      <td>WA</td>\n",
       "      <td>149</td>\n",
       "      <td>Special educational needs teacher</td>\n",
       "      <td>0</td>\n",
       "      <td>Stephanie Gill</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38859492057661</td>\n",
       "      <td>fraud_Lind-Buckridge</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>220.11</td>\n",
       "      <td>M</td>\n",
       "      <td>Malad City</td>\n",
       "      <td>ID</td>\n",
       "      <td>4154</td>\n",
       "      <td>Nature conservation officer</td>\n",
       "      <td>0</td>\n",
       "      <td>Edward Sanchez</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3534093764340240</td>\n",
       "      <td>fraud_Kutch, Hermiston and Farrell</td>\n",
       "      <td>gas_transport</td>\n",
       "      <td>45.00</td>\n",
       "      <td>M</td>\n",
       "      <td>Boulder</td>\n",
       "      <td>MT</td>\n",
       "      <td>1939</td>\n",
       "      <td>Patent attorney</td>\n",
       "      <td>0</td>\n",
       "      <td>Jeremy White</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>375534208663984</td>\n",
       "      <td>fraud_Keeling-Crist</td>\n",
       "      <td>misc_pos</td>\n",
       "      <td>41.96</td>\n",
       "      <td>M</td>\n",
       "      <td>Doe Hill</td>\n",
       "      <td>VA</td>\n",
       "      <td>99</td>\n",
       "      <td>Dance movement psychotherapist</td>\n",
       "      <td>0</td>\n",
       "      <td>Tyler Garcia</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1852389</th>\n",
       "      <td>30560609640617</td>\n",
       "      <td>fraud_Reilly and Sons</td>\n",
       "      <td>health_fitness</td>\n",
       "      <td>43.77</td>\n",
       "      <td>M</td>\n",
       "      <td>Luray</td>\n",
       "      <td>MO</td>\n",
       "      <td>519</td>\n",
       "      <td>Town planner</td>\n",
       "      <td>0</td>\n",
       "      <td>Michael Olson</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1852390</th>\n",
       "      <td>3556613125071656</td>\n",
       "      <td>fraud_Hoppe-Parisian</td>\n",
       "      <td>kids_pets</td>\n",
       "      <td>111.84</td>\n",
       "      <td>M</td>\n",
       "      <td>Lake Jackson</td>\n",
       "      <td>TX</td>\n",
       "      <td>28739</td>\n",
       "      <td>Futures trader</td>\n",
       "      <td>0</td>\n",
       "      <td>Jose Vasquez</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1852391</th>\n",
       "      <td>6011724471098086</td>\n",
       "      <td>fraud_Rau-Robel</td>\n",
       "      <td>kids_pets</td>\n",
       "      <td>86.88</td>\n",
       "      <td>F</td>\n",
       "      <td>Burbank</td>\n",
       "      <td>WA</td>\n",
       "      <td>3684</td>\n",
       "      <td>Musician</td>\n",
       "      <td>0</td>\n",
       "      <td>Ann Lawson</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1852392</th>\n",
       "      <td>4079773899158</td>\n",
       "      <td>fraud_Breitenberg LLC</td>\n",
       "      <td>travel</td>\n",
       "      <td>7.99</td>\n",
       "      <td>M</td>\n",
       "      <td>Mesa</td>\n",
       "      <td>ID</td>\n",
       "      <td>129</td>\n",
       "      <td>Cartographer</td>\n",
       "      <td>0</td>\n",
       "      <td>Eric Preston</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1852393</th>\n",
       "      <td>4170689372027579</td>\n",
       "      <td>fraud_Dare-Marvin</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>38.13</td>\n",
       "      <td>M</td>\n",
       "      <td>Edmond</td>\n",
       "      <td>OK</td>\n",
       "      <td>116001</td>\n",
       "      <td>Media buyer</td>\n",
       "      <td>0</td>\n",
       "      <td>Samuel Frey</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1852394 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   cc_num                            merchant        category  \\\n",
       "0        2703186189652095          fraud_Rippin, Kub and Mann        misc_net   \n",
       "1            630423337322     fraud_Heller, Gutmann and Zieme     grocery_pos   \n",
       "2          38859492057661                fraud_Lind-Buckridge   entertainment   \n",
       "3        3534093764340240  fraud_Kutch, Hermiston and Farrell   gas_transport   \n",
       "4         375534208663984                 fraud_Keeling-Crist        misc_pos   \n",
       "...                   ...                                 ...             ...   \n",
       "1852389    30560609640617               fraud_Reilly and Sons  health_fitness   \n",
       "1852390  3556613125071656                fraud_Hoppe-Parisian       kids_pets   \n",
       "1852391  6011724471098086                     fraud_Rau-Robel       kids_pets   \n",
       "1852392     4079773899158               fraud_Breitenberg LLC          travel   \n",
       "1852393  4170689372027579                   fraud_Dare-Marvin   entertainment   \n",
       "\n",
       "            amt gender            city state  city_pop  \\\n",
       "0          4.97      F  Moravian Falls    NC      3495   \n",
       "1        107.23      F          Orient    WA       149   \n",
       "2        220.11      M      Malad City    ID      4154   \n",
       "3         45.00      M         Boulder    MT      1939   \n",
       "4         41.96      M        Doe Hill    VA        99   \n",
       "...         ...    ...             ...   ...       ...   \n",
       "1852389   43.77      M           Luray    MO       519   \n",
       "1852390  111.84      M    Lake Jackson    TX     28739   \n",
       "1852391   86.88      F         Burbank    WA      3684   \n",
       "1852392    7.99      M            Mesa    ID       129   \n",
       "1852393   38.13      M          Edmond    OK    116001   \n",
       "\n",
       "                                       job  is_fraud       full_name  month  \\\n",
       "0                Psychologist, counselling         0  Jennifer Banks      1   \n",
       "1        Special educational needs teacher         0  Stephanie Gill      1   \n",
       "2              Nature conservation officer         0  Edward Sanchez      1   \n",
       "3                          Patent attorney         0    Jeremy White      1   \n",
       "4           Dance movement psychotherapist         0    Tyler Garcia      1   \n",
       "...                                    ...       ...             ...    ...   \n",
       "1852389                       Town planner         0   Michael Olson     12   \n",
       "1852390                     Futures trader         0    Jose Vasquez     12   \n",
       "1852391                           Musician         0      Ann Lawson     12   \n",
       "1852392                       Cartographer         0    Eric Preston     12   \n",
       "1852393                        Media buyer         0     Samuel Frey     12   \n",
       "\n",
       "        day  hour  age_at_transaction  \n",
       "0         1     0                31.0  \n",
       "1         1     0                41.0  \n",
       "2         1     0                57.0  \n",
       "3         1     0                52.0  \n",
       "4         1     0                33.0  \n",
       "...      ..   ...                 ...  \n",
       "1852389  31    23                55.0  \n",
       "1852390  31    23                21.0  \n",
       "1852391  31    23                39.0  \n",
       "1852392  31    23                55.0  \n",
       "1852393  31    23                28.0  \n",
       "\n",
       "[1852394 rows x 15 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display dataframe to check changes\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+\n",
      "|summary|               amt|          city_pop|age_at_transaction|\n",
      "+-------+------------------+------------------+------------------+\n",
      "|  count|           1852394|           1852394|           1852394|\n",
      "|   mean| 70.06356747538618| 88643.67450931066| 46.26536201261719|\n",
      "| stddev|159.25397477398303|301487.61834365036|17.412095278772394|\n",
      "|    min|               1.0|                23|              14.0|\n",
      "|    max|           28948.9|           2906700|              96.0|\n",
      "+-------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#show statistic of the data we want\n",
    "df.select('amt','city_pop','age_at_transaction').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to calculate quantiles for numerical variables\n",
    "def get_quantile(column):\n",
    "    quantile = df.approxQuantile([column], [0.25, 0.5, 0.75], 0)\n",
    "    quantile_25 = quantile[0][0]\n",
    "    quantile_50 = quantile[0][1]\n",
    "    quantile_75 = quantile[0][2]\n",
    "    print(f'quantiles for {column} column:')\n",
    "    print('    quantile_25: '+str(quantile_25))\n",
    "    print('    quantile_50: '+str(quantile_50))\n",
    "    print('    quantile_75: '+str(quantile_75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantiles for amt column:\n",
      "    quantile_25: 9.64\n",
      "    quantile_50: 47.45\n",
      "    quantile_75: 83.1\n",
      "quantiles for city_pop column:\n",
      "    quantile_25: 741.0\n",
      "    quantile_50: 2443.0\n",
      "    quantile_75: 20328.0\n",
      "quantiles for age_at_transaction column:\n",
      "    quantile_25: 33.0\n",
      "    quantile_50: 44.0\n",
      "    quantile_75: 57.0\n"
     ]
    }
   ],
   "source": [
    "# use created function to get quantiles for amt, city_pop, and age_at_transaction columns\n",
    "get_quantile('amt')\n",
    "get_quantile('city_pop')\n",
    "get_quantile('age_at_transaction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to find fraud cases for each categorical variable\n",
    "def top_fraud(column):\n",
    "    df.groupBy(column).agg(sf.sum('is_fraud').alias(\"sum_fraud\") \\\n",
    "                               ,sf.round(sf.avg('is_fraud'), 4).alias('avg_fraud')) \\\n",
    "                                .sort(sf.desc(\"avg_fraud\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+---------+\n",
      "|            merchant|sum_fraud|avg_fraud|\n",
      "+--------------------+---------+---------+\n",
      "|   fraud_Kozey-Boehm|       60|   0.0218|\n",
      "|fraud_Herman, Tre...|       38|   0.0203|\n",
      "|    fraud_Terry-Huel|       56|   0.0196|\n",
      "|fraud_Kerluke-Abs...|       50|    0.019|\n",
      "|fraud_Mosciski, Z...|       53|   0.0188|\n",
      "|fraud_Schmeler, B...|       52|   0.0187|\n",
      "|     fraud_Kuhic LLC|       53|   0.0186|\n",
      "|      fraud_Jast Ltd|       51|   0.0185|\n",
      "|fraud_Langworth, ...|       52|   0.0185|\n",
      "|fraud_Boyer-Reichert|       51|   0.0184|\n",
      "|fraud_Romaguera, ...|       51|   0.0184|\n",
      "|fraud_Heathcote, ...|       51|   0.0183|\n",
      "|   fraud_Goyette Inc|       50|    0.018|\n",
      "| fraud_Lemke-Gutmann|       50|   0.0179|\n",
      "|  fraud_Medhurst PLC|       48|   0.0175|\n",
      "| fraud_Heathcote LLC|       47|   0.0169|\n",
      "|  fraud_Rau and Sons|       60|   0.0169|\n",
      "|fraud_Greenholt, ...|       46|   0.0168|\n",
      "|fraud_Moore, Dibb...|       31|   0.0168|\n",
      "|   fraud_Durgan-Auer|       31|   0.0168|\n",
      "+--------------------+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# correlation between merchant and fraud cases\n",
    "top_fraud('merchant')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appears to be no correlation between the merchant and the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+---------+\n",
      "|      category|sum_fraud|avg_fraud|\n",
      "+--------------+---------+---------+\n",
      "|  shopping_net|     2219|   0.0159|\n",
      "|      misc_net|     1182|    0.013|\n",
      "|   grocery_pos|     2228|   0.0126|\n",
      "|  shopping_pos|     1056|   0.0063|\n",
      "| gas_transport|      772|   0.0041|\n",
      "|      misc_pos|      322|   0.0028|\n",
      "|        travel|      156|   0.0027|\n",
      "|   grocery_net|      175|   0.0027|\n",
      "| personal_care|      290|   0.0022|\n",
      "| entertainment|      292|   0.0022|\n",
      "|     kids_pets|      304|   0.0019|\n",
      "|   food_dining|      205|   0.0016|\n",
      "|health_fitness|      185|   0.0015|\n",
      "|          home|      265|   0.0015|\n",
      "+--------------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# correlation between transaction category and fraud cases\n",
    "top_fraud('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appears to be a slight correlation between the transaction type and the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+---------+\n",
      "|month|sum_fraud|avg_fraud|\n",
      "+-----+---------+---------+\n",
      "|    2|      853|   0.0087|\n",
      "|    1|      849|   0.0081|\n",
      "|    3|      938|   0.0065|\n",
      "|    5|      935|   0.0064|\n",
      "|   10|      838|   0.0061|\n",
      "|    9|      758|   0.0054|\n",
      "|    4|      678|    0.005|\n",
      "|   11|      682|   0.0048|\n",
      "|    6|      821|   0.0047|\n",
      "|    8|      797|   0.0045|\n",
      "|    7|      652|   0.0038|\n",
      "|   12|      850|    0.003|\n",
      "+-----+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# correlation between transaction month and fraud cases\n",
    "top_fraud('month')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appears to be some correlation between the month and the number of fraud cases. As can be shown above, the fraud cases are appaarently I get to you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---------+\n",
      "|day|sum_fraud|avg_fraud|\n",
      "+---+---------+---------+\n",
      "| 12|      402|    0.007|\n",
      "| 11|      397|   0.0069|\n",
      "| 20|      419|   0.0069|\n",
      "| 19|      364|   0.0063|\n",
      "| 25|      369|   0.0063|\n",
      "| 31|      217|    0.006|\n",
      "|  3|      345|   0.0059|\n",
      "|  8|      373|   0.0058|\n",
      "|  4|      335|   0.0058|\n",
      "| 17|      328|   0.0056|\n",
      "|  2|      316|   0.0053|\n",
      "| 10|      307|   0.0052|\n",
      "| 28|      346|   0.0052|\n",
      "|  7|      332|   0.0052|\n",
      "| 14|      324|   0.0051|\n",
      "| 13|      308|   0.0051|\n",
      "|  5|      286|    0.005|\n",
      "| 18|      285|    0.005|\n",
      "| 23|      290|   0.0049|\n",
      "| 27|      297|   0.0049|\n",
      "+---+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# correlation between transaction day of month and fraud cases\n",
    "top_fraud('day')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appears to be no significant correlations with the fraud cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+---------+\n",
      "|hour|sum_fraud|avg_fraud|\n",
      "+----+---------+---------+\n",
      "|  22|     2481|    0.026|\n",
      "|  23|     2442|   0.0255|\n",
      "|   0|      823|   0.0136|\n",
      "|   1|      827|   0.0135|\n",
      "|   3|      803|   0.0132|\n",
      "|   2|      793|    0.013|\n",
      "|   5|       80|   0.0013|\n",
      "|  18|      111|   0.0012|\n",
      "|   7|       72|   0.0012|\n",
      "|  20|       98|   0.0011|\n",
      "|  14|      100|   0.0011|\n",
      "|  21|      101|   0.0011|\n",
      "|  19|      105|   0.0011|\n",
      "|  15|      100|   0.0011|\n",
      "|  13|       94|    0.001|\n",
      "|  17|       94|    0.001|\n",
      "|   8|       59|    0.001|\n",
      "|   4|       61|    0.001|\n",
      "|  16|       97|    0.001|\n",
      "|  11|       59|    0.001|\n",
      "+----+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# correlation between transaction time and fraud cases\n",
    "top_fraud('hour')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appears to be a correlation between the time of the day when the fraud cases. Fraud cases appear to be more prevalant during night time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------+---------+\n",
      "|age_at_transaction|sum_fraud|avg_fraud|\n",
      "+------------------+---------+---------+\n",
      "|              96.0|        7|    0.028|\n",
      "|              77.0|       84|   0.0133|\n",
      "|              87.0|       48|   0.0115|\n",
      "|              92.0|       77|   0.0112|\n",
      "|              18.0|       45|   0.0101|\n",
      "|              63.0|      193|   0.0099|\n",
      "|              79.0|      115|   0.0098|\n",
      "|              71.0|      127|   0.0095|\n",
      "|              78.0|       97|   0.0094|\n",
      "|              86.0|       52|   0.0094|\n",
      "|              56.0|      194|   0.0087|\n",
      "|              57.0|      212|   0.0082|\n",
      "|              58.0|      222|   0.0081|\n",
      "|              80.0|       73|   0.0078|\n",
      "|              76.0|       47|   0.0078|\n",
      "|              59.0|      175|   0.0077|\n",
      "|              62.0|      136|   0.0077|\n",
      "|              89.0|       43|   0.0076|\n",
      "|              53.0|      238|   0.0074|\n",
      "|              85.0|       53|   0.0072|\n",
      "+------------------+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# correlation between age of customer during transaction and fraud cases\n",
    "top_fraud('age_at_transaction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appears to be a correlation between the age and the fraud cases appear to be good. As the favorite targets to fraud where, older and younger customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+---------+\n",
      "|state|sum_fraud|avg_fraud|\n",
      "+-----+---------+---------+\n",
      "|   DE|        9|      1.0|\n",
      "|   RI|       15|   0.0201|\n",
      "|   AK|       50|   0.0169|\n",
      "|   OR|      197|   0.0075|\n",
      "|   NH|       79|   0.0067|\n",
      "|   VA|      273|   0.0065|\n",
      "|   TN|      159|   0.0064|\n",
      "|   NE|      216|   0.0063|\n",
      "|   MN|      280|   0.0062|\n",
      "|   NY|      730|   0.0061|\n",
      "|   DC|       31|    0.006|\n",
      "|   KS|      193|   0.0059|\n",
      "|   NV|       47|   0.0058|\n",
      "|   CO|      115|   0.0058|\n",
      "|   SC|      236|   0.0057|\n",
      "|   ME|      134|   0.0057|\n",
      "|   MS|      169|   0.0056|\n",
      "|   IN|      217|   0.0055|\n",
      "|   GA|      204|   0.0055|\n",
      "|   WI|      228|   0.0055|\n",
      "+-----+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# correlation between state the transaction happened in and fraud cases\n",
    "top_fraud('state')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be a strong correlation as the state 'DE' saw a 100% fraud cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+---------+\n",
      "|          city|sum_fraud|avg_fraud|\n",
      "+--------------+---------+---------+\n",
      "|         Wever|        6|      1.0|\n",
      "|    Coulee Dam|       15|      1.0|\n",
      "|       Norfolk|        7|      1.0|\n",
      "|West Frankfort|       13|      1.0|\n",
      "|    Wappapello|        8|      1.0|\n",
      "|        Crouse|        8|      1.0|\n",
      "|        Roland|       11|      1.0|\n",
      "|        Waukau|       10|      1.0|\n",
      "|       Seattle|       19|      1.0|\n",
      "|       Ashland|       10|      1.0|\n",
      "|   Springville|       12|      1.0|\n",
      "|      Streator|        7|      1.0|\n",
      "|    East China|        9|      1.0|\n",
      "|        Angwin|       10|      1.0|\n",
      "|   Chattanooga|        7|      1.0|\n",
      "|         Craig|       14|      1.0|\n",
      "|       Medford|        9|      1.0|\n",
      "|     Senatobia|       10|      1.0|\n",
      "|    Brookfield|        9|      1.0|\n",
      "|   Orange Park|       10|      1.0|\n",
      "+--------------+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# correlation between city the transaction happened in and fraud cases\n",
    "top_fraud('city')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appears to be a strong correlation as a lot of the cities had a 100% fraud rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+---------+\n",
      "|                 job|sum_fraud|avg_fraud|\n",
      "+--------------------+---------+---------+\n",
      "| Information officer|        8|      1.0|\n",
      "|Contracting civil...|        7|      1.0|\n",
      "|Accountant, chart...|       11|      1.0|\n",
      "|    Industrial buyer|       10|      1.0|\n",
      "|  Veterinary surgeon|        8|      1.0|\n",
      "|   Warehouse manager|        7|      1.0|\n",
      "|              Dancer|       19|      1.0|\n",
      "|   Personnel officer|       12|      1.0|\n",
      "|Forest/woodland m...|        9|      1.0|\n",
      "|      Engineer, site|       12|      1.0|\n",
      "|Sales promotion a...|       14|      1.0|\n",
      "|           Homeopath|       11|      1.0|\n",
      "|   Software engineer|       11|      1.0|\n",
      "|     Engineer, water|        8|      1.0|\n",
      "|         Ship broker|        7|      1.0|\n",
      "|Armed forces tech...|        8|      1.0|\n",
      "|Broadcast journalist|        9|      1.0|\n",
      "|Operational inves...|       11|      1.0|\n",
      "|     Legal secretary|       12|      1.0|\n",
      "|Air traffic contr...|       17|      1.0|\n",
      "+--------------------+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# correlation between customer job and fraud cases\n",
    "top_fraud('job')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same with the city column, job column seems to have a high correlation with the fraud cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+---------+\n",
      "|gender|sum_fraud|avg_fraud|\n",
      "+------+---------+---------+\n",
      "|     M|     4752|   0.0057|\n",
      "|     F|     4899|   0.0048|\n",
      "+------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# correlation between customer gender and fraud cases\n",
    "top_fraud('gender')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appears to be no significant between gender and the fraud cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2093078472297254"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate correlation between amount of transaction and fraud cases\n",
    "df.stat.corr('is_fraud','amt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+---------+\n",
      "|    amt_bucket|sum_fraud|avg_fraud|\n",
      "+--------------+---------+---------+\n",
      "|   1000 - 1500|     1226|    0.339|\n",
      "|    500 - 1000|     3458|   0.2136|\n",
      "| Less than 500|     4967|   0.0027|\n",
      "|More than 1500|        0|      0.0|\n",
      "+--------------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create transaction amount buckets\n",
    "df = Bucketizer(splits=[-float('inf'), 0, 500, 1000, 1500, float('inf')],\n",
    "                 inputCol='amt', outputCol='amt_bin'\n",
    "        ).transform(df)\n",
    "\n",
    "buckets = {1.0: \"Less than 500\", 2.0:\"500 - 1000\", 3.0: \"1000 - 1500\", 4.0: \"More than 1500\"}\n",
    "udf_foo = sf.udf(lambda x: buckets[x], types.StringType())\n",
    "df = df.withColumn(\"amt_bucket\", udf_foo(\"amt_bin\"))\n",
    "\n",
    "# correlation between amount bucket and fraud cases\n",
    "top_fraud('amt_bucket')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define string columns to index\n",
    "string_columns = ['category','city','state','job']\n",
    "\n",
    "# Create an indexer that will create new columns with numeric index values\n",
    "for col in string_columns:\n",
    "    df = StringIndexer(inputCol=col, outputCol=f'{col}_idx').fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- amt: double (nullable = true)\n",
      " |-- is_fraud: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- age_at_transaction: double (nullable = true)\n",
      " |-- category_idx: double (nullable = false)\n",
      " |-- city_idx: double (nullable = false)\n",
      " |-- state_idx: double (nullable = false)\n",
      " |-- job_idx: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop columns with low correlation with target variable\n",
    "df = df.drop('cc_num','merchant','category','gender', 'city', 'state', 'city_pop','job','full_name','amt_bucket','amt_bin')\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all column to integer\n",
    "df_columns = df.columns\n",
    "for column in df_columns:\n",
    "    df = df.withColumn(column, df[column].cast(types.IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+--------+\n",
      "|features                                     |is_fraud|\n",
      "+---------------------------------------------+--------+\n",
      "|[4.0,1.0,1.0,0.0,31.0,11.0,173.0,12.0,129.0] |0       |\n",
      "|[107.0,1.0,1.0,0.0,41.0,1.0,27.0,29.0,64.0]  |0       |\n",
      "|[220.0,1.0,1.0,0.0,57.0,6.0,795.0,45.0,426.0]|0       |\n",
      "|[45.0,1.0,1.0,0.0,52.0,0.0,661.0,39.0,205.0] |0       |\n",
      "|[41.0,1.0,1.0,0.0,33.0,10.0,224.0,13.0,284.0]|0       |\n",
      "+---------------------------------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create an assembler object\n",
    "assembler = VectorAssembler(inputCols=['amt', 'month', 'day', 'hour', 'age_at_transaction',\n",
    "                                       'category_idx', 'city_idx', 'state_idx','job_idx'],\n",
    "                            outputCol='features')\n",
    "\n",
    "# Consolidate predictor columns\n",
    "df_assembled = assembler.transform(df)\n",
    "\n",
    "# Check the resulting column\n",
    "df_assembled.select('features','is_fraud').show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataframe to main dataframe ratio: 0.8001586055666343\n"
     ]
    }
   ],
   "source": [
    "# split data into train and test\n",
    "df_train, df_test = df_assembled.randomSplit([0.8, 0.2], seed=100)\n",
    "\n",
    "# # Check that training set has around 80% of records\n",
    "training_ratio = df_train.count() / df_assembled.count()\n",
    "print(f'Train dataframe to main dataframe ratio: {training_ratio}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------------------------------------------+\n",
      "|is_fraud|prediction|probability                               |\n",
      "+--------+----------+------------------------------------------+\n",
      "|0       |0.0       |[0.9963781971970396,0.0036218028029604303]|\n",
      "|0       |0.0       |[0.9997172268656229,2.8277313437705625E-4]|\n",
      "|0       |0.0       |[0.9997172268656229,2.8277313437705625E-4]|\n",
      "|0       |0.0       |[0.9997172268656229,2.8277313437705625E-4]|\n",
      "|0       |0.0       |[0.9997172268656229,2.8277313437705625E-4]|\n",
      "+--------+----------+------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a classifier object and fit to the training data\n",
    "tree = DecisionTreeClassifier(featuresCol = 'features', labelCol='is_fraud')\n",
    "tree_model = tree.fit(df_train)\n",
    "\n",
    "# Create predictions for the testing data and take a look at the predictions\n",
    "prediction = tree_model.transform(df_test)\n",
    "prediction.select('is_fraud', 'prediction', 'probability').show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+\n",
      "|is_fraud|prediction| count|\n",
      "+--------+----------+------+\n",
      "|       1|       0.0|  1010|\n",
      "|       0|       0.0|368046|\n",
      "|       1|       1.0|  1000|\n",
      "|       0|       1.0|   129|\n",
      "+--------+----------+------+\n",
      "\n",
      "\n",
      "accuracy = 0.9969231600416009\n",
      "sensitivity = 0.4975124378109453\n",
      "specificity = 0.9996496231411692\n",
      "precision = 0.8857395925597874\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction.groupBy('is_fraud', 'prediction').count().show()\n",
    "\n",
    "# # Calculate the elements of the confusion matrix\n",
    "TN = prediction.filter('prediction = 0 AND is_fraud = prediction').count()\n",
    "TP = prediction.filter('prediction = 1 AND is_fraud = prediction').count()\n",
    "FN = prediction.filter('prediction = 0 AND is_fraud = 1').count()\n",
    "FP = prediction.filter('prediction = 1 AND is_fraud = 0').count()\n",
    "\n",
    "# Calculate confusion matrix metrics\n",
    "accuracy = (TN + TP) / (TN + TP + FN + FP)\n",
    "sensitivity = TP / (TP + FN)\n",
    "specificity = TN / (TN + FP)\n",
    "precision = TP / (TP + FP)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"\"\"\n",
    "accuracy = {accuracy}\n",
    "sensitivity = {sensitivity}\n",
    "specificity = {specificity}\n",
    "precision = {precision}\n",
    "\"\"\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CART model performed well as it was able to identify close to 50% of the fraud cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+\n",
      "|is_fraud|prediction| count|\n",
      "+--------+----------+------+\n",
      "|       1|       0.0|  2010|\n",
      "|       0|       0.0|367992|\n",
      "|       0|       1.0|   183|\n",
      "+--------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create classifier object and train on training data\n",
    "logistic_model = LogisticRegression(featuresCol = 'features', labelCol='is_fraud').fit(df_train)\n",
    "\n",
    "# Create a predictions for the test data and show confusion matrix\n",
    "prediction = logistic_model.transform(df_test)\n",
    "prediction.groupBy(\"is_fraud\", \"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "accuracy = 0.9940759350054702\n",
      "sensitivity = 0.0\n",
      "specificity = 0.9995029537584029\n",
      "precision = 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Calculate the elements of the confusion matrix\n",
    "TN = prediction.filter('prediction = 0 AND is_fraud = prediction').count()\n",
    "TP = prediction.filter('prediction = 1 AND is_fraud = prediction').count()\n",
    "FN = prediction.filter('prediction = 0 AND is_fraud = 1').count()\n",
    "FP = prediction.filter('prediction = 1 AND is_fraud = 0').count()\n",
    "\n",
    "# Calculate confusion matrix metrics\n",
    "accuracy = (TN + TP) / (TN + TP + FN + FP)\n",
    "sensitivity = TP / (TP + FN)\n",
    "specificity = TN / (TN + FP)\n",
    "precision = TP / (TP + FP)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"\"\"\n",
    "accuracy = {accuracy}\n",
    "sensitivity = {sensitivity}\n",
    "specificity = {specificity}\n",
    "precision = {precision}\n",
    "\"\"\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression model performed badly as it was not able to detect any of the fraud cases"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
